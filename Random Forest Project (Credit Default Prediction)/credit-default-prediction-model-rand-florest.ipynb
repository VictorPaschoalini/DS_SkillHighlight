{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3df0a08",
   "metadata": {
    "papermill": {
     "duration": 0.003177,
     "end_time": "2023-06-20T16:02:07.546125",
     "exception": false,
     "start_time": "2023-06-20T16:02:07.542948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Business Context\n",
    "\n",
    "In the financial industry, the ability to accurately predict credit default risk is paramount. This task is traditionally handled by credit scoring models that are used to determine a customer's likelihood of defaulting on their credit obligations. Credit risk models are a critical component in the decision-making process for financial institutions when issuing loans or credit cards.\n",
    "\n",
    "Default prediction models, like the one built in this project, help these institutions by automating the process of assessing credit risk, leading to more efficient and reliable credit decision-making. By accurately predicting whether a borrower is likely to default on their credit payments, lenders can manage risk more effectively, minimizing losses due to bad loans.\n",
    "\n",
    "A well-performing default prediction model can greatly benefit a company in several ways:\n",
    "\n",
    "- Risk Mitigation: Accurate prediction of default allows the bank to manage its risk better, resulting in a healthier loan portfolio.\n",
    "\n",
    "- Profitability: By identifying less risky customers, the bank can focus its resources on customers who are more likely to fulfill their credit obligations, leading to more profitable lending.\n",
    "\n",
    "- Customer Relationship Management: An accurate credit scoring model can also help in maintaining good customer relations. If a model can identify customers at risk of defaulting, early intervention strategies can be put in place. This not only helps the customer avoid a default but can also strengthen the customer-bank relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d34f9f6",
   "metadata": {
    "papermill": {
     "duration": 0.002996,
     "end_time": "2023-06-20T16:02:07.552679",
     "exception": false,
     "start_time": "2023-06-20T16:02:07.549683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Python Model for Credit Default Prediction\n",
    "\n",
    "This Python script is an end-to-end solution for predicting credit card payment defaults. It includes a few significant steps that aim to increase the prediction accuracy, from initial data preparation to model training and evaluation.\n",
    "\n",
    "#### Reading the dataset\n",
    "\n",
    "The dataset from the location \"/kaggle/input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv\" is loaded into a DataFrame. The column \"default.payment.next.month\" is renamed to \"default\" for easier access.\n",
    "\n",
    "#### Preparing the data\n",
    "\n",
    "The data is divided into features (X) and target (y). The 'default' column is the target variable which we aim to predict.\n",
    "\n",
    "#### Splitting the data\n",
    "\n",
    "The data is divided into training and test sets with a ratio of 70:30.\n",
    "\n",
    "#### Addressing class imbalance\n",
    "\n",
    "Since the class distribution in the dataset is skewed, we use SMOTE (Synthetic Minority Over-sampling Technique) to oversample the minority class in the training data. This technique helps improve the performance of the model on minority class.\n",
    "\n",
    "#### Preprocessing the data\n",
    "\n",
    "StandardScaler is used to standardize the features by removing the mean and scaling to unit variance. This is important because different features might use different scales, and we want to ensure that our model doesn't unfairly emphasize some features over others.\n",
    "\n",
    "#### Training the model\n",
    "\n",
    "A Random Forest classifier model is trained on the processed data. To get the best results, a grid search is performed over specified parameter values for the model. This involves training multiple models with different combinations of hyperparameters and choosing the one that performs best.\n",
    "\n",
    "#### Making predictions\n",
    "\n",
    "The trained model is used to make predictions on the test set.\n",
    "\n",
    "#### Evaluating the model performance\n",
    "\n",
    "Finally, the model's performance is evaluated using classification report, confusion matrix, and AUC-ROC score. These metrics provide detailed insights into the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b69ddcae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-20T16:02:07.561583Z",
     "iopub.status.busy": "2023-06-20T16:02:07.561061Z",
     "iopub.status.idle": "2023-06-20T16:10:20.624384Z",
     "shell.execute_reply": "2023-06-20T16:10:20.622231Z"
    },
    "papermill": {
     "duration": 493.072353,
     "end_time": "2023-06-20T16:10:20.628417",
     "exception": false,
     "start_time": "2023-06-20T16:02:07.556064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the dataset...\n",
      "Preparing the data...\n",
      "Splitting the data...\n",
      "Addressing class imbalance with SMOTE...\n",
      "Preprocessing the data...\n",
      "Training the model (this may take a while)...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] END ......................max_depth=2, n_estimators=100; total time=   2.0s\n",
      "[CV] END ......................max_depth=2, n_estimators=100; total time=   2.1s\n",
      "[CV] END ......................max_depth=2, n_estimators=100; total time=   2.0s\n",
      "[CV] END ......................max_depth=2, n_estimators=100; total time=   2.1s\n",
      "[CV] END ......................max_depth=2, n_estimators=100; total time=   2.0s\n",
      "[CV] END ......................max_depth=2, n_estimators=200; total time=   4.0s\n",
      "[CV] END ......................max_depth=2, n_estimators=200; total time=   4.0s\n",
      "[CV] END ......................max_depth=2, n_estimators=200; total time=   4.0s\n",
      "[CV] END ......................max_depth=2, n_estimators=200; total time=   4.0s\n",
      "[CV] END ......................max_depth=2, n_estimators=200; total time=   4.0s\n",
      "[CV] END ......................max_depth=2, n_estimators=300; total time=   6.1s\n",
      "[CV] END ......................max_depth=2, n_estimators=300; total time=   5.9s\n",
      "[CV] END ......................max_depth=2, n_estimators=300; total time=   5.9s\n",
      "[CV] END ......................max_depth=2, n_estimators=300; total time=   5.9s\n",
      "[CV] END ......................max_depth=2, n_estimators=300; total time=   5.9s\n",
      "[CV] END ......................max_depth=2, n_estimators=400; total time=   8.1s\n",
      "[CV] END ......................max_depth=2, n_estimators=400; total time=   7.8s\n",
      "[CV] END ......................max_depth=2, n_estimators=400; total time=   7.7s\n",
      "[CV] END ......................max_depth=2, n_estimators=400; total time=   7.7s\n",
      "[CV] END ......................max_depth=2, n_estimators=400; total time=   7.8s\n",
      "[CV] END ......................max_depth=4, n_estimators=100; total time=   3.2s\n",
      "[CV] END ......................max_depth=4, n_estimators=100; total time=   3.2s\n",
      "[CV] END ......................max_depth=4, n_estimators=100; total time=   3.1s\n",
      "[CV] END ......................max_depth=4, n_estimators=100; total time=   3.1s\n",
      "[CV] END ......................max_depth=4, n_estimators=100; total time=   3.2s\n",
      "[CV] END ......................max_depth=4, n_estimators=200; total time=   6.3s\n",
      "[CV] END ......................max_depth=4, n_estimators=200; total time=   6.3s\n",
      "[CV] END ......................max_depth=4, n_estimators=200; total time=   6.3s\n",
      "[CV] END ......................max_depth=4, n_estimators=200; total time=   6.2s\n",
      "[CV] END ......................max_depth=4, n_estimators=200; total time=   6.3s\n",
      "[CV] END ......................max_depth=4, n_estimators=300; total time=   9.4s\n",
      "[CV] END ......................max_depth=4, n_estimators=300; total time=   9.8s\n",
      "[CV] END ......................max_depth=4, n_estimators=300; total time=   9.5s\n",
      "[CV] END ......................max_depth=4, n_estimators=300; total time=   9.4s\n",
      "[CV] END ......................max_depth=4, n_estimators=300; total time=   9.5s\n",
      "[CV] END ......................max_depth=4, n_estimators=400; total time=  12.7s\n",
      "[CV] END ......................max_depth=4, n_estimators=400; total time=  12.4s\n",
      "[CV] END ......................max_depth=4, n_estimators=400; total time=  12.5s\n",
      "[CV] END ......................max_depth=4, n_estimators=400; total time=  12.5s\n",
      "[CV] END ......................max_depth=4, n_estimators=400; total time=  12.5s\n",
      "[CV] END ......................max_depth=6, n_estimators=100; total time=   4.3s\n",
      "[CV] END ......................max_depth=6, n_estimators=100; total time=   4.3s\n",
      "[CV] END ......................max_depth=6, n_estimators=100; total time=   4.2s\n",
      "[CV] END ......................max_depth=6, n_estimators=100; total time=   4.2s\n",
      "[CV] END ......................max_depth=6, n_estimators=100; total time=   4.1s\n",
      "[CV] END ......................max_depth=6, n_estimators=200; total time=   8.5s\n",
      "[CV] END ......................max_depth=6, n_estimators=200; total time=   8.5s\n",
      "[CV] END ......................max_depth=6, n_estimators=200; total time=   8.4s\n",
      "[CV] END ......................max_depth=6, n_estimators=200; total time=   8.5s\n",
      "[CV] END ......................max_depth=6, n_estimators=200; total time=   8.7s\n",
      "[CV] END ......................max_depth=6, n_estimators=300; total time=  13.0s\n",
      "[CV] END ......................max_depth=6, n_estimators=300; total time=  12.9s\n",
      "[CV] END ......................max_depth=6, n_estimators=300; total time=  13.0s\n",
      "[CV] END ......................max_depth=6, n_estimators=300; total time=  12.7s\n",
      "[CV] END ......................max_depth=6, n_estimators=300; total time=  12.9s\n",
      "[CV] END ......................max_depth=6, n_estimators=400; total time=  17.4s\n",
      "[CV] END ......................max_depth=6, n_estimators=400; total time=  17.2s\n",
      "[CV] END ......................max_depth=6, n_estimators=400; total time=  17.0s\n",
      "[CV] END ......................max_depth=6, n_estimators=400; total time=  17.0s\n",
      "[CV] END ......................max_depth=6, n_estimators=400; total time=  16.9s\n",
      "Making predictions...\n",
      "Evaluating the model performance...\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85      7040\n",
      "           1       0.48      0.55      0.51      1960\n",
      "\n",
      "    accuracy                           0.77      9000\n",
      "   macro avg       0.67      0.69      0.68      9000\n",
      "weighted avg       0.78      0.77      0.78      9000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5856 1184]\n",
      " [ 878 1082]]\n",
      "AUC-ROC:\n",
      "0.6919294990723562\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Reading the dataset\n",
    "print(\"Reading the dataset...\")\n",
    "df = pd.read_csv('/kaggle/input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv')\n",
    "df.rename(columns={'default.payment.next.month':'default'}, inplace=True)\n",
    "\n",
    "# Splitting the data into features (X) and target (y)\n",
    "print(\"Preparing the data...\")\n",
    "X = df.drop('default', axis=1)\n",
    "y = df['default']\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "print(\"Splitting the data...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Addressing class imbalance with SMOTE\n",
    "print(\"Addressing class imbalance with SMOTE...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Preprocessing the data (standardization)\n",
    "print(\"Preprocessing the data...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Training a Random Forest model with hyperparameter tuning\n",
    "print(\"Training the model (this may take a while)...\")\n",
    "parameters = {'n_estimators': [100, 200, 300, 400], 'max_depth': [2, 4, 6]}\n",
    "model = GridSearchCV(RandomForestClassifier(random_state=42), parameters, verbose=2)\n",
    "model.fit(X_train_scaled, y_train_smote)\n",
    "\n",
    "# Making predictions on the test set\n",
    "print(\"Making predictions...\")\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluating the model performance\n",
    "print(\"Evaluating the model performance...\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"AUC-ROC:\")\n",
    "print(roc_auc_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec74bf",
   "metadata": {
    "papermill": {
     "duration": 0.008347,
     "end_time": "2023-06-20T16:10:20.646077",
     "exception": false,
     "start_time": "2023-06-20T16:10:20.637730",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Results and Next Steps\n",
    "\n",
    "The performance of our random forest model on the test data is promising, with an AUC-ROC score of approximately 0.692. This score indicates that the model can effectively distinguish between the classes. It means that the model has a 69.2% chance of scoring a randomly chosen positive instance higher than a randomly chosen negative one.\n",
    "\n",
    "The precision and recall for the class 1 (default) are relatively lower than for class 0 (non-default). The model has a precision of 0.48 for class 1, meaning that it correctly identifies 48% of the actual defaults. The recall for class 1 is 0.55, implying that the model identifies 55% of the total defaults.\n",
    "\n",
    "There are several potential next steps to further improve this model:\n",
    "\n",
    "- Feature Engineering: We could create new features or modify existing ones to potentially improve the model's performance.\n",
    "\n",
    "- Hyperparameter Tuning: We could fine-tune the model's hyperparameters further. While we've already conducted a grid search for 'n_estimators' and 'max_depth', there are other parameters, like 'min_samples_split', 'min_samples_leaf', or 'max_features', that we could tune as well.\n",
    "\n",
    "- Different Models: We could try other machine learning models such as Gradient Boosting or Support Vector Machines to see if they perform better on this problem.\n",
    "\n",
    "- Ensemble Methods: We could also consider using ensemble methods to combine predictions from multiple models. This often improves performance.\n",
    "\n",
    "- Handling Class Imbalance in Other Ways: Our target variable is imbalanced, which can bias the model's predictions towards the majority class. Techniques such as SMOTE (already used), undersampling, or oversampling can be used to address this.\n",
    "\n",
    "- Deep Learning: As a more complex solution, we could use deep learning models, which may capture more intricate patterns in the data. These models might be especially beneficial if we have a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eae0c2",
   "metadata": {
    "papermill": {
     "duration": 0.008377,
     "end_time": "2023-06-20T16:10:20.663536",
     "exception": false,
     "start_time": "2023-06-20T16:10:20.655159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 508.852653,
   "end_time": "2023-06-20T16:10:21.901224",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-20T16:01:53.048571",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
